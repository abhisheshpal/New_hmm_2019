#! /usr/bin/env python
# -*- coding: utf-8 -*-
"""
Created on: Day Mon DD HH:MM:SS YYYY

@author: gpdas
"""
import numpy as np
import matplotlib.pyplot as plt
import hmms
from pprint import pformat

class HMModel(object):
    """
    """
    def __init__(self, n_states, from_file, f_name=None, trans_rate_mat=None, obs_prob_mat=None, init_state_prob=None):
        """
        Keywork arguments:

        trans_rate_mat -- transition rate marix, (n_states x n_states)
        obs_prob_mat -- observation probability matrix,
                        it indicates given an observation what is the probability that it is correct,
                        (n_states x n_states+1) additional state as a means for querying
        init_state_prob -- array of initial state probabilities, (n_states)
        """
        self.n_states = n_states
        if from_file:
            assert f_name is not None
            # create CtHMM from file
            self.from_file(f_name)
        else:
            assert trans_rate_mat is not None
            assert obs_prob_mat is not None
            assert init_state_prob is not None
            # Create CtHMM by given parameters
            self._model = hmms.CtHMM(trans_rate_mat, obs_prob_mat, init_state_prob)
        print('Q=%s,\nB=%s,\nPi=%s' % self._model.params)

    def from_file(self, f_name):
        """Load model object from a file
        Set & Get Parameters:Later you can always set parameters with triple of methods corresponding to the constructors.
        chmm.set_params(Q,B,Pi)
        chmm.set_params_random(3,3)
        chmm.set_params_from_file( "state2_cthmm.npz" )
        """
        self._model = hmms.CtHMM.from_file(f_name)

    def to_file(self, f_name):
        """save cthmm model to a npz file
        """
        self._model.save_params(f_name)

    def generate_random(self, sample_len, sample_step , verbose=False):
        """generates predictions from a random initial state (decided based on the state probs)
        """
        # sample uniformly
        t_seq = range(0,sample_len, sample_step)

        t_seq, s_seq, e_seq = self._model.generate(len(t_seq), time=t_seq)

        #resize plot
        plt.rcParams['figure.figsize'] = [20,20]

        hmms.plot_hmm(s_seq, e_seq, time=t_seq )
        if verbose:
            print('t_seq',t_seq)

        return (t_seq, s_seq, e_seq)

    def predict(self, obs=np.array([0,1,2,3,4]), predict_time=20.0, verbose=False):
        """ predict function that takes some initial observations so far

        Keyword arguments:

        predict_time - look ahead time (i.e. the time we look into the future based on the last observation)
        """
        e_seq = np.array(obs)
        # assume the observations were made 1 second apart
        t_seq = np.array(range(0,len(e_seq)))

        # Now the predict step
        # the last observation codes for unknown,"abusing" the Viterbi algorithm to provide predictions
        # purely on the transition model
        e_seq[-1:] = self.n_states

        # set the last "unknown" observation time:
        t_seq[-1] = t_seq[-2] + predict_time

        if verbose:
            print('t_seq, e_seq', t_seq, e_seq)

        # run Viterbi algorithm for the CtHMM
        (log_prob, s_seq) =  self._model.viterbi( t_seq, e_seq )

        # We can also query the state distribution for the entire sequence
        log_prob_table = self._model.states_confidence( t_seq, e_seq )
        post_distribution = np.exp( log_prob_table[-1] )

        if verbose:
            # Let's print the most likely state sequence
            hmms.plot_hmm( s_seq, e_seq, time = t_seq )
            print( "found state sequence: \n", s_seq )
            print( "predicted state looking %f seconds into the future: %d" % (predict_time, s_seq[-1]) )
            print( "Probability of being generated by the found state sequence:", np.exp( log_prob ) )
#            print( "state probs: \n", post_distribution)

        uniform = np.array([1.0 / self.n_states] * self.n_states)   # get the 1 X N_nodes matrix of uniform distribution
        # Now KL divergence is used to get the difference between the two distributions--
        # (here btween uniform and post_distribution) and is denoted by D_KL = (post_distribution || uniform) =
        # itergral of (p(x)* log((px)/q(x))
        # KL divergence: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Basic_example
        # KL can be used here as a measure of certainty to some extend, used to compare against uniform distribution,
        # the longer into the future the prediction, the closer KL is to 0
        D_KL = np.sum(np.multiply(post_distribution, np.log(np.divide(post_distribution, uniform))))

        if verbose:
            print("Kullbackâ€“Leibler divergence (high is good in this case, as I compare against uniform): %f" % D_KL)

        return (s_seq[-1], D_KL, post_distribution)

    def check_prediction_probs(self, obs, forecast_max, forecast_steps, verbose=False):
        """
        Keyword arguments:

        obs -- initial observations
        forecast_max -- max time to forecast
        forecast_steps -- time steps to be taken to reach forecast_max
        """
        posteriors = []
        states = {}
        kls = []
        times = []

        # try forecasting for several time steps:
        for i in np.linspace(1, forecast_max, forecast_steps):  # Return evenly spaced numbers over a specified interval (start,stop, Num = int).
            (state, kl, posterior) = self.predict(
                                                         obs,
                                                         predict_time = i,
                                                         verbose = False
                                                         )
            times.append(i)
            posteriors.append(posterior)
            states[i] = state
            kls.append(kl)

#         plot the posterior probabilities
        plt.plot(times, posteriors, '-x')

#         plot the KL divergence (certainty)
        plt.plot(times, kls, linewidth=3)

        legend = list(range(0, self.n_states))
        legend.append('KL')
        plt.legend(legend)
        plt.show()

        if verbose:
            print('state predictions:\n%s' % pformat(states))

        return times, states, kls, posteriors
